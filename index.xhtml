<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb">
  <head>
    <!--nomodify-->
    <meta name="style" content="cs" />
    <title>COMSM1052 - Foundations of Practice-oriented AI - University of Bristol</title>
    <style>
      table { border-spacing: 2px; border-collapse: separate; }
      td { margin: 1px; padding: 5px; }
      table.blank  {vertical-align: top; text-align: left; background: #f1f1f1;}
      td.blank  {vertical-align: top; text-align: center; background: #cccccc;}
      tr.blank  {vertical-align: top; text-align: left; background: #cccccc;}
      td.Rui {vertical-align: top; text-align: left; background: #FFE5B4;}
      td.lecture {vertical-align: top; text-align: left; background: #ccffc1;}
      td.dropin {vertical-align: top; text-align: left; background: #ffaaaa;}
      td.lab {vertical-align: top; text-align: left; background: #bbbbee;}
    </style>
  </head>

  <body>
    <div id="wrap">
      <a href="https://www.ole.bris.ac.uk/ultra/courses/_260177_1/cl/outline">BLACKBOARD PAGE</a> |
      <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?unitCode=COMSM0152" target="_blank">UNIT INFO</a> |
      <h1>COMSM0152 - Foundations of ProAI</h1>


      <link rel="stylesheet" href="simple.css" />
      <a id="info"></a>
      <hr/>

       <!--h2>News</h2> 

       <p>This page is currently under construction and all materials
	 accessible from it are liable to change.
       </p>
       
       <hr/-->
       
      <h2>Unit Information</h2>

      <p>
	This unit introduces students to a range of methodologies and applications across the field of AI. Each week will focus on a different theme, with three activities:
        <ol>
         <li><strong>Required reading</strong> -- background reading to introduce the topic of the week</li>
         <li><strong>Guest seminars</strong> (TB2: Mondays, 1300-1400, Queen's 1.58) -- a guest speaker will present an introduction to their research area, connected to the week's topic</li>
         <li><strong>Group discussion</strong> (TB2: Fridays, 1000-1100, Queen's 1.68) -- student led discussion on the required reading and guest seminars. This is a chance to go over the important concepts and raise questions. The sessions will be chaired by a student, to facilitate the discussion (they are not required to present the topic).
         <a href="https://uob-my.sharepoint.com/:x:/g/personal/es1595_bristol_ac_uk/EUF7hhAa0eNJmVn2QZsHWEsBvZxGzn7xzpYS4Qdrd32GJA?e=Rb4Wnj">Link to spreadsheet with session chairs</a></li>
         <li><strong>AI Lab Lunch and Learn Seminars: </strong> (usually Wednesdays at 1330-1500, Queen's 1.07) -- these sessions are not exclusive to the CDT and we invite the whole AI Lab and beyond, with visiting researchers as well as our local speakers giving talks, and, importantly, free pizza. Many foundational and applied AI topics are covered here, so it is worth attending.</li>
        </ol>
      </p>

      Bear with us while we organise the schedule below -- Topics are likely to change a bit as we arrange with our guest speakers.

      Please do give us feedback at any time on how we can make the unit work better! In particular, if you find some topics assume prior knowledge you don't have, or would like to go further into certain topics, let us know. 
      Please see below for some great all-round <strong>textbooks on AI</strong>, which may help you fill in some knowledge gaps...

      We also have a <strong>Team</strong> on MS Teams. Please do use it to ask us questions, discuss AI topics you are interested in, post interesting blogs, videos or papers, etc...


      <hr/>

      <h2> Staff</h2>

      <p>
	<a href="https://jcussens.github.io/"
	     target="_blank">James Cussens (JC)
	</a>
	<a href="https://uob-nlp.github.io/"
	     target="_blank">Edwin Simpson (ES)
	</a>
      </p>

      <hr/>

      <h2>Unit Materials</h2>


<table border="1" style="border-collapse:collapse" cellspacing="1" cellpadding="2">

  <tr>
    <td><i>Week</i></td>
    <td><i>Reading (to complete before Monday's lecture)</i></td>
    <td><i>Guest Seminar</i></td>
    <td><i>Discussion chair (<a href="https://uob-my.sharepoint.com/:x:/g/personal/es1595_bristol_ac_uk/Eeu6TYNbcZlOpzxH_GotwkMBi7KREWMM29mXyDbh1q_GFA?e=aTkIu8">Link to spreadsheet with session chairs</a>)</i></td>
    <td><i>Lecturer responsible</i></td>
  </tr>
  <tr><h3>TB1</h3></tr>

  <tr>
    <td>1  (w/c 22/09/25) </td>
    <td class="lecture">
      Embeddings and Deep Learning for NLP
      <ul>
       <li><a href="https://uob-my.sharepoint.com/:p:/g/personal/es1595_bristol_ac_uk/EbdNaSkSvftLmJ_6RcRYDq0B7XdMdgfAPPM646gZUg5Jig?e=jXbtxZ">Slides from the seminar</a></li>
       <li><a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">Embeddings</a> </li>
       <li><a href="https://arxiv.org/abs/1810.04805">BERT, Devlin et al., 2018</a> </li>
       <li><a href="https://uob-my.sharepoint.com/:b:/g/personal/es1595_bristol_ac_uk/ER4QOWEKbAVEpG7LkMH9uX0BA__U6-UKIjyHH46sfNNqPw?e=VbiMbvf">Optimising Factual Consistency in Summarisation via Preference Learning from Multiple Imperfect Metrics: 
</a> This is our recent paper to be presented at EMNLP Findings in November, showing how we are using embeddings and transfer learning to improve text generation with small LLMs (please read before next Friday).</li>
      </ul>
    </td>
    <td class="lecture">
      Edwin Simpson
    </td>
    <td class="lecture">

    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>2  (w/c 29/09/25) </td>
    <td class="lecture">
      Attention and Transformers. Both of the following are really good introductions to attention and transformers 
      - you can pick one to read, or go through both if that helps your understanding. Please cover one of these before Monday:
      <ul> 
       <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
       <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Jurafsky and Martin Chapter 9, "The Transformer"</a></li>
      </ul>
      For the Friday reading group, please also read: 
      <ul>
       <li><a href="https://arxiv.org/pdf/1804.02516">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</a></li>
       <li><a href="https://arxiv.org/pdf/2210.09461">(and OPTIONALLY, for the keen) TOKEN MERGING: YOUR VIT BUT FASTER </a></li>
      </ul>
    </td>
    <td class="lecture">
      Mike Wray
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>3  (w/c 06/10/25) </td>
    <td class="lecture">
      Large language models: 
      <ul>
       <li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">Jurafsky and Martin, Chapter 10</a></li>
       <li><a href="https://web.stanford.edu/~jurafsky/slp3/12.pdf">Jurafsky and Martin, Chapter 12, sections up to and inluding 12.4</a></li>
       <li><a href="https://arxiv.org/pdf/1611.01368">Assessing the ability of LSTMs to learn syntax-specific dependencies</a>, Linzen et al. (2016), TACL</li>
       <li><a href="https://arxiv.org/pdf/2303.00077">Beyond the limitations of any imaginable mecahnism: large language models and Psycholinguistics</a>, Houghton, Kazanina and Sukumaran, Behavioral and Brain Sciences, (2023). </li>
      </ul>
    </td>
    <td class="lecture">
      Conor Houghton -- linguistic perspectives on LLMs
    </td>    
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>4  (w/c 13/10/25) </td>
    <td class="lecture">
      Data and labels for image and Video understanding: 
      <ul>
       <li><a href="https://arxiv.org/abs/1409.0575">ImageNet</a>, the competition that kick-started the deep learning revolution in Computer Vision </li> 
       <li><a href="https://www.cvlibs.net/publications/Geiger2013IJRR.pdf">Kitti</a>, a highly impactful dataset in vision and robotics. </li>
       <li>Optional extra: The winner of the ImageNet 2015 competition, <a href="https://ieeexplore.ieee.org/document/7780459">ResNet.</a> </li>
      </ul>
    </td>
    <td class="lecture">
      Dima Damen -- tutorial on data and labels for video understanding
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>5  (w/c 20/10/25) </td>
<td class="lecture">
      Causality:
      <ul>
        <li> Chapter 1 of <a href="http://bayes.cs.ucla.edu/PRIMER/">
        Causal Inference in Statistics: A Primer</a> by Pearl, Glymour
        and Jewell. You can skip Section 1.3 if you're already familiar
        with the material it contains. 
        </li>
        <li>
          <a href="https://doi.org/10.1093/ije/dyw114">The tale wagged
            by the DAG: broadening the scope of causal inference and
            explanation for epidemiology</a> by Krieger and Davey-Smith
        </li>
        <li>
          <a href="https://doi.org/10.1093/ije/dyy068">Comments on:
            The tale wagged by the DAG</a> by Pearl
        </li>
        <li>
        <a href="https://doi.org/10.1093/ije/dyy071">Reply to Pearl:
        Algorithm of the truth vs real-world science</a> by Krieger
        and Davey-Smith
        </li>
        <li>Supplementary reading: Schölkopf et al, <a href="https://arxiv.org/abs/2102.11107">Towards Causal Representation Learning</a>
        </li>
      </ul>
    </td>
    <td class="lecture">
      James Cussens
    </td>
    <td class="lecture">
      
    </td>
     <td>JC</td>
 </tr>

  <tr>
    <td>6  (w/c 27/10/25) </td>
    <td>
      CONSOLIDATION WEEK
    </td>
    <td>
      (no seminar/reading group) 
    </td>
    <td>
    </td>
  </tr>

  <tr>
    <td>7  (w/c 03/11/25) </td>
    <td class="lecture">
      Philosophy of AI
      <!-- <ul>
        <li> <a href="https://link.springer.com/article/10.1007/s11023-018-9479-0">An
        Analysis of the Interaction Between Intelligent Software
            Agents and Human Users</a> Christopher Burr, Nello
          Cristianini and James Ladyman</li>
        <li> <a href="https://arxiv.org/pdf/2408.07144">Language
            Models as Models of Language</a> Raphaël Millière</li>
        <li> <a href="https://www.arxiv.org/pdf/2408.09544">No Such
        Thing as a General Learner: Language models and their dual
        optimization</a> Emmanuel Chemla and Ryan M. Nefdt</li>
      </ul> -->
    </td>
    <td class="lecture">
      James Ladyman
    </td>
    <td class="lecture">
      
    </td>
     <td>JC</td>
 </tr>

  <tr>
    <td>8  (w/c 10/11/25) </td>
    <td class="lecture">
      Bias, fairness and transparency
      <!-- <ul>
        <li>
          European High-level expert group on AI, <a href="https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf">Ethics Guidelines for Trustworthy AI</a>, read sections 4 and 5 of the assessment list on pp.28-30.  
        </li>
        <li>
          Rebcca Burns, <a href="https://jacobin.com/2023/06/artificial-intelligence-corporate-landlords-tenants-screening-crime-racism">Artificial Intelligence is driving discrimination in the housing market</a>, read up to the section "Not an excuse for lawbreaking behaviour."
        </li>
        <li>
          AI Fairness 360: read the short Credit Scoring tutorial via
          <a href="https://github.com/Trusted-AI/AIF360/blob/main/examples/README.md">this link</a>
        </li>
        <li>
          <a href="Bias Fairness Transparency 2024.pptx">Miranda's slides</a>
        </li>
      </ul> -->
    </td>
    <td class="lecture">
      Miranda Mowbray 
    </td>      
    <td class="lecture">
      
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>9  (w/c 17/11/25) </td>
    <td class="lecture">
      Shaping Healthcare Data with AI: Imaging and Language Models
      <ul>
        <li>Imaging: <a href="https://doi.org/10.7554/eLife.99848.4">CellSeg3D, Self-supervised 3D cell segmentation for fluorescence microscopy</a></li>
        <li><a href="https://www.youtube.com/watch?v=1z5rwWr5tlI">Generative Diffusion Models for Medical Imaging</a></li>
        <li>Language: <a href="https://doi.org/10.1016/j.compbiomed.2025.110246">Detecting the clinical features of difficult-to-treat depression using synthetic data from large language models</a></li>
        <li><a href="https://www.youtube.com/watch?v=c8rdC_rmy6M">Training LLMs with Synthetic Data (How NVIDIA Trained Nemotron)</a></li>
      </ul>
    </td>
    <td class="lecture">
       Qiang Liu
    </td>
    <td class="lecture">
      
    </td>
     <td>ES</td>
 </tr>

  <tr>
    <td>10  (w/c 24/11/25) </td>
    <td class="lecture">
<!-- Bayesian inference and decision making --> 
    Robust AI and Generalisation 
      <!-- <ul>
        <li><a href="https://www.lesswrong.com/posts/XTXWPQSEgoMkAupKt/an-intuitive-explanation-of-bayes-s-theorem">An intuitive explanation of Bayes theorem (blog post)</a>
        </li>
        <li><a href="https://arxiv.org/pdf/1906.02691">An introduction
        to variational autoencoders (Sections 1 and 2)</a>
        </li>
      </ul> -->
    </td>
    <td class="lecture">
      Gabriel Oliveira
      <!-- Laurence Aitchison (<a href="bayes_practice_oriented_ai.pptx">slides</a>)  -->
    </td>
    
    <td class="lecture">
      
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>11  (w/c 01/12/25) </td>
    <td class="lecture">
      Ethical and regulatory frameworks of AI
      <!-- <ul>
        <li>Charlesworth, A. <a href="https://www-cambridge-org.bris.idm.oclc.org/core/books/datadriven-personalisation-in-markets-politics-and-law/regulating-algorithmic-assemblages-looking-beyond-corporatist-ai-ethics/1AC7B521C51FB038DD49E6C034E7D5E7">Regulating Algorithmic Assemblages: Looking Beyond Corporatist AI Ethics</a>, in Kohl, U. and Eisler, J. (eds.) (2022) Data-Driven Personalisation and the Law. Cambridge University Press.
        </li>
        <li>Charlesworth A. <a href="2024-11-01 Chapter - AI Regulation in the UK.pdf">AI Regulation in the UK</a> in  Raposo, V.L. (ed.) The European Artificial Intelligence Act, Springer (forthcoming 2025).
        </li>
        <li>
      Additional Reading: Guihot, M., Matthew, A. F., and Suzor, N. P., (2020)
      <a href="https://scholarship.law.vanderbilt.edu/jetlaw/vol20/iss2/2">Nudging Robots: Innovative Solutions to Regulate Artificial Intelligence</a>, Vanderbilt Journal of Entertainment and Technology Law 20, 385-456.
        </li>
        <li>
          Additional Reading: <a href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj">EU Artificial Intelligence Act</a> (Regulation (EU) 2024/1689) Chapter II
          - Prohibited AI Practices, Article 5 ;  Chapter III-  High
          Risk AI Systems, Arts. 6-17; Chapter V - General-Purpose AI
          Models, Arts. 51-56; Chapter X - Codes of Conduct &amp;
          Guidelines, Art.95.
        </li>
      </ul>-->
    </td> 
    <td class="lecture">
      TBC
      <!-- Andrew Charlesworth  -->
    </td>
    <td class="lecture">
      
    </td>
     <td>JC</td>
 </tr>

  <tr>
    <td>12  (w/c 08/12/25) </td>
    <td>
      Assessment period
    </td>
    <td >
      TB1 essay due this week
    </td>
    <td>
    </td>
  </tr>

  <tr><h3>TB2</h3></tr>

  <tr>
    <td>13  (w/c 19/01/26) </td>
    <!-- <td class="lecture">
      TBC: Causality or Discrete and continuous optimisation?
      <ul>
        <li>
          Peter Bartlett. The Science of Modern Machine Learning: New
          Opportunities for Theory (link to talk sent privately)
        </li>
        <li>
          Bill
          Cook. <a href="https://www.youtube.com/watch?v=5VjphFYQKj8">The Traveling Salesman Problem: Postcards from the Edge of Impossibility</a>
        </li>
      </ul> 
    </td>
    <td class="lecture">
      James Cussens
    </td>
    <td class="lecture">
      
    </td> -->
<td class="lecture">
      Knowledge representation and reasoning
      <ul>
        <li>
          <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/download/1029/947">R. Davis, H. Shrobe, and P. Szolovits. What is a Knowledge Representation?  AI Magazine, 14(1):17-33, 1993 (16pp. + references)</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2308.04161">Delgrande, J. P., Glimm, B., Meyer, T., Truszczynski, M., and Wolter, F. (2023). Current and future challenges in knowledge representation and reasoning. arXiv preprint arXiv:2308.04161 (30pp. + appendices and an extensive bibliography)</a>
        </li>
          <li>Additional
        reading: <a href="https://link.springer.com/article/10.1007/s13218-023-00824-7">What
        are Non-classical Logics and Why Do We Need Them? An Extended
            Interview with Dov Gabbay and Leon van der Torre</a>
        </li>
      </ul>
    </td>
    <td class="lecture">
      Oliver Ray
      <!-- Peter Flach -->
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>14  (w/c 26/01/26) </td>
    <td class="lecture">
      Reinforcement learning (TBC)
      <!-- <ul>
        <li>
           <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Chapter 1 of Sutton and Barto's book on reinforcement learning</a>. 
           It would be useful to read this before the Monday session as it gives an introduction to the topic. However, Taku is not assuming prior knowledge of RL in his talk. 
        </li>
        <li>
          <a href="https://arxiv.org/pdf/1312.5602">Playing Atari with Deep Reinforcement Learning </a>, a classic deep RL paper
        </li>
        <li>
          <a href="https://arxiv.org/abs/2209.03993">Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL</a> by Yamagata, Khalil, and Santos-Rodriguez
        </li>
     </ul> -->
    </td>
    <td class="lecture">
      <!-- Taku Yamagata  -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>15  (w/c 02/02/26) </td>
	<td class="lecture">
    Robotics (TBC)
	  <!-- Tactile robotics: 
	  Nathan has kindly provided us a sneak preview of two of his papers: 
	  <a href="https://www.ole.bris.ac.uk/webapps/blackboard/content/listContentEditable.jsp?content_id=_9314854_1&amp;course_id=_260177_1" title="Link to Nathan's preview papers on Blackboard. Requires login.">you can find them here on Blackboard</a> 
	  (you'll need to log in first). Please don't share the papers as they have not yet been published.  -->
	</td>
    <td class="lecture">
      <!-- Nathan Lepora  -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>16  (w/c 09/02/26) </td>
    <td class="lecture">
      Multi-agent systems (TBC)
      <!-- <ul>
        <li>
          Please skim through this chapter: Gerhard Weiss (2013). 2013. Intelligent agents. In Multiagent
          Systems, 2nd ed., pages 3-50. MIT Press.  [eBook available
          through <a href="https://research-ebsco-com.bris.idm.oclc.org/c/pvb2jm/search/details/peytl27vs5?db=nlebk">University
          of Bristol library]</a>
        </li>
        <li>
          Pradeep
          K. Murukannaiah, Nirav Ajmeri, Catholijn Jonker, and
          Munindar P. Singh. <a href="https://niravajmeri.github.io/docs/AAMAS20-EthicalMAS-BlueSky.pdf">New Foundations of Ethical Multiagent
            Systems.</a> Proceedings of the 19th International Conference on
          Autonomous Agents and Multiagent Systems (AAMAS), Blue Sky
          Idea Track, Auckland, May 2020, pages 1706-1710.
        </li>
        <li>
          Jessica
          Woodgate, Paul Marshall, and Nirav Ajmeri. <a href="https://niravajmeri.github.io/docs/AAAI25-Rawle.pdf">Operationalising
          Rawlsian Ethics for Fairness in Norm-Learning
            Agents</a>. Proceedings of the 39th AAAI Conference on
          Artificial Intelligence (AAAI), Philadelphia, Feb 2025,
          pages 1-9.
        </li>
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Nirav Ajmeri  -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>17  (w/c 16/02/26) </td>
    <td class="lecture">
      Weak supervision (TBC) 
      <!-- <ul>
        <li>An introduction to the topic: <a href="https://snorkel.ai/data-centric-ai/weak-supervision/">Snorkel AI webpages/videos</a> </li>
        <li><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5951191/">Snorkel: rapid training data creation with weak supervision (2017). </a></li>
        <li><a href="https://jmlr.org/papers/volume11/raykar10a/raykar10a.pdf">Learning from Crowds (Raykar et al., 2010)</a> – a canonical paper on treating crowdsourced labels as weak supervision </li>
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Raul Santos-Rodriguez -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>18  (w/c 23/02/26) </td>
    <td>
      CONSOLIDATION WEEK
    </td>
    <td>
      No seminar/reading group
    </td>
    <td>
    </td>
  </tr>

  <tr>
    <td>19  (w/c 02/03/26) </td>
    <td class="lecture">
      	Learning from Temporal Data (TBC)
    <!-- <ul>
      <li>
        Have a look at
        this <a href="https://www.timeseriesclassification.com/">central
          resource for time series classification datasets and
          methods</a>
      </li>
      <li>
        <a href="https://link.springer.com/article/10.1007/S10618-016-0483-9">The great time series classification bake off</a>
      </li>
      <li>
        <a href="https://arxiv.org/pdf/1809.04356">Deep learning for time series classification: a review</a>
      </li>
      <li>Supplementary
        reading: <a href="https://dl.acm.org/doi/10.1145/3637528.3671600">EEG2Rep:
          Enhancing Self-supervised EEG Representation Through Informative
          Masked Inputs</a>
      </li>
      <li>Supplementary
        reading: <a href="https://link.springer.com/article/10.1007/s10618-020-00701-z">ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels</a>
      </li>
      <li>Supplementary
        reading: <a href="https://link.springer.com/article/10.1007/s10618-020-00727-3">The great multivariate time series classification bake off</a>
      </li>
    </ul> -->
    </td>
    <td class="lecture">
      <!-- Zahraa Abdallah -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>20  (w/c 09/03/26) </td>
    <td class="lecture">
      Explainable and interpretable AI (TBC)
      <!-- <ul>
        <li>
            <a href="https://uob-my.sharepoint.com/:b:/g/personal/dw24954_bristol_ac_uk/EbLxfauH94FJn55mXl2ioTgBfKlozj2IgVAGxsfhMyLSfA?e=KGDPy7/">Counterfactual explanations and how to find them: literature review and benchmarking</a>
        </li>
        <li>
          <a href="https://uob-my.sharepoint.com/:b:/g/personal/dw24954_bristol_ac_uk/Ec2M1BF98gpFn9gjFOGwXngBWAx-FZ47AFfUXPt7vniLJQ?e=lK2Us0">If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques</a>
        </li>
        <li>
          <a href="https://ieeexplore.ieee.org/document/9895252">Explainable AI for Time Series Classification: A Review, Taxonomy and Research Directions</a>
        </li>
        <li>Supplementary
          reading: <a href="https://uob-my.sharepoint.com/:b:/g/personal/dw24954_bristol_ac_uk/ESLftrZ89EBAkoUSBVPcnzQBUZ4qA1NO3TozeV79A_PCMg?e=dIZsAg">EEG2Rep:
            Counterfactual Explanations for Machine Learning: A Review</a>
        </li>
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Weiru Liu -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>21  (w/c 16/03/26) </td>
    <td class="lecture">
      Privacy (TBC)
      <!-- <ul>
        <li>
          <a href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/ai-and-data-protection-risk-toolkit/">The
            AI &amp; data protection risk toolkit from the UK
            Information Commissioner's Office, focussing on column I,
            Practical Steps to Reduce the Risk.</a> (Select the "Risk
            toolkit" tab.)
        </li>
        <li>
          Steps 5 and 6
          of <a href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/how-do-we-do-a-dpia/">How
          do we do a DPIA?</a>
        </li>
        <li>
          <a href="https://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google">This
          Wes Davis article</a>
        </li>   
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Miranda Mowbray -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>JC</td>
  </tr>

  <tr>
    <td>Easter Vacation  (w/c 23/03/26) </td>    
  </tr>

  <tr>
    <td>22  (w/c 13/04/26) </td>
    <td class="lecture">
      Robust AI or MLOps and deploying AI in production (TBC)
      <!-- <ul>
        <li>
          <a href="https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2024.1342937/full">Resilience-aware MLOps for AI-based medical diagnostic system (2024)</a>
        </li>
              <li>
          <a href="https://ieeexplore.ieee.org/abstract/document/10081336">Machine Learning Operations (MLOps): Overview, Definition, and Architecture (2023)</a>
        </li>
              <li>
          Have a browse of this (a huge resource where you can find information about any MLOps topic of interest: <a href="https://github.com/visenger/awesome-mlops?tab=readme-ov-file. ">Awesome MLOps Github Repo</a>
        </li>
        <li>
          Extra: <a href="https://www.researchgate.net/profile/Naveen-Kodakandla/publication/388192754_Scaling_AI_responsibly_Leveraging_MLOps_for_sustainable_machine_learning_deployments/links/678e8ea795e02f182ea3f8fd/Scaling-AI-responsibly-Leveraging-MLOps-for-sustainable-machine-learning-deployments.pdf">Scaling AI responsibly: Leveraging MLOps for sustainable machine learning deployments (2024)</a>
        </li>   
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Jose Navarro, Cleo AI (external guest speaker)  -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

  <tr>
    <td>23  (w/c 20/04/26) </td>
    <td class="lecture">
      Human-in-the-loop AI -- design and evaluation (TBC)
       <!-- <ul>
        <li>
          <a href="https://edrev.asu.edu/index.php/ER/article/download/2197/651">Cathy O’Neil, C. (2016) Weapons of Math Destruction. Penguin</a>: Introduction, Chapter 1.
        </li>
              <li>
          <a href="https://scholar.google.de/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=crawford+atlas+of+ai&amp;btnG=">Crawford, K. (2021) Atlas of AI. Yale</a>: Chapter 2 Labor, Chapter 4 Classification.
        </li>
              <li>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3589953">Ghosh, P. L. Posner, K. Hyland, S. Van Cleve W, Bristow M, Long DR, Palla K, Nair B, Fong, C., Pauldine, R, Vavilala, M.and Kenton O'Hara.K (2023) Framing Machine Learning Opportunities for Hypotension Prediction in Perioperative Care: A Socio-technical Perspective. ACM Transactions on Computer-Human Interaction, 30(5)</a>
        </li>
        <li>
          <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9074793/pdf/main.pdf">Palla K, Hyland SL, Posner K, Ghosh P, Nair B, Bristow M, Paleva Y, Williams B, Fong C, Van Cleve W, Long DR, Pauldine R, O'Hara K, Takeda K, Vavilala MS. (2022) Intraoperative prediction of post anaesthesia care unit hypotension. Br J Anaesth,128(4):623-635.</a>
        </li>   
      </ul> -->
    </td>
    <td class="lecture">
      <!-- Kenton O'Hara -->
       TBC
    </td>
    <td class="lecture">
      
    </td>
    <td>ES</td>
  </tr>

</table>

      <hr/>

      <h2>Assessment Details</h2>

    <p>
      Deadlines: at end of TB1 and end of TB2, dates TBC
    </p>
    
      <p>
      After each Teaching Block students submit an essay of about 5,000 words (10 pages) on a research topic jointly chosen by them and their Academic Mentor. 
      The essay should describe the background, state of the art, and open challenges with regard to the chosen topic. 
      Each essay is assessed on a pass/fail basis in terms of scholarly content and academic writing. 
      Narrative feedback is also provided, indicating strong points as well as areas for improvement. Passing the unit requires passing both essays.	
      More guidance will be provided by the unit lecturers during the first term. 
      </p>
      
      <hr/>

<!--h2>Lab Work</h2>
The labs are formative assessements which we strongly encourage you to
complete. Note that these were designed to help your understanding of
ML methods.

<h3>Using the machines in Queen's 180</h3>

      <p>
	Most (not all) of the software we use in this unit is supplied as Python
	packages bundled with the Anaconda Python package manager. If you are
	using the machines in Queen's 180 to do the lab exercises (as
	opposed to using your own machine) you need to do the
	following to start using Anaconda.
      </p>

      <ol>
	<li>
	  Make sure the machine you are using is running Linux (reboot
	  if necessary). 
	</li>
	<li>
	  Open up a Terminal window so you have access to the Linux
	  command line.
	</li>
	<li>
	  Enter the following at the command line: <code>module load
	    anaconda</code>
	</li>
	<li>
	  Enter the following at the command line: <code>conda init</code>
	</li>
	<li>
	  The <code>conda init</code> command alters
	  the <code>.bashrc</code> file in your home directory to
	  ensure that you are using the version of Python provided by Anaconda (have
	  a look at that file if you want). To get the changes
	  in <code>.bashrc</code> to take effect the easiest option is
	  just to kill your terminal window and start up a new one; so
	  do that. Note that you only need to run <code>conda
	  init</code> once (unless you change the shell you're using). 
	</li>
	<li>
	  Once you have a new terminal you should notice that your
	  prompt has changed to include the text <code>(base)</code>
	  that indicates
	  which <a href="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">Anaconda
	  environment</a> is currently active.
	</li>
	<li>
	  To access the ML software for this unit you need to use
	  the <code>coms30035</code> Anaconda Virtual Environment. To
	  do this type <code>conda activate coms30035</code> at the
	  command line. Your shell prompt should change from
	  having <code>(base)</code> to having <code>(coms30035)</code>. 
	</li>
	<li>
	  If you call Python from this shell it should use
	  the <code>coms30035</code> virtual environment. Common ways
	  of using Python include:
	  <ol>
	    <li> Typing <code>python</code> at the prompt to start the
	    Python interpreter.
	    </li>
	    <li> Typing <code>python somescript.py</code> to execute a
	    Python script (in this example with the name <code>somescript.py</code>).
	    </li>
	    <li> Typing <code>jupyter lab</code> at the prompt to start JupyterLab.
	    </li>
	    <li> Typing <code>jupyter lab somenotebook.ipynb</code> to
	    inspect (and possibly execute) a
	    JupyterLab notebook (in this example with the name <code>somenotbook.ipynb</code>).
	    </li>
	  </ol>
	</li>
      </ol>
      
      <h3>Using your own machine</h3>
      
      <p>If you want to do lab exercises on your own machine then you
	should
	install <a href="https://www.anaconda.com">Anaconda</a> on
	it. If you run into installation problems then feel free to
	ask the Teaching
	Staff on the unit for help, but we can't guarantee to solve
	them.
      </p-->

<h2>Text books</h2>

      <ol>
          <li> <b>Bishop, C. M., Pattern recognition and machine learning (2006)</b>. 
          This is one of the best ML textbooks and will provide a solid foundation across many aspects of ML. 
          The book is freely available <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">here</a>.</li>

          <li> <b>Russell, S. and Norvig, P., Artificial Intelligence, A Modern Approach, 4th Edition (2020)</b>. 
          The canonical introduction to AI, 3rd edition available at <a href="https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf">here</a>.</li>

          <li> <b>Jurafsky, D. and Martin, J.H., Speech and Language Processing, 3rd edition drafts (2024)</b>. A great NLP textbook, but very readable and
          an interesting way to think about the challenges of designing AI systems in general. Also good for showing a contrast between deep learning 
          and feature engineering approaches. Online only, <a href="https://web.stanford.edu/~jurafsky/slp3/">here</a>.</li>

          <li> <b>Murphy, K., Probabilistic Machine Learning: An
          Introduction (2022)</b> and <b>Murphy, K., Probabilistic
          Machine Learning: Advanced Topics (2023)</b>. A more recent ML
          textbook with particularly good coverage of
          probabilistic methods, freely available via <a href="https://probml.github.io/pml-book/">here</a>.</li>
      </ol>

      Some of you may prefer <strong>videos</strong> to books. Youtube has many video lectures, including <a href="https://www.youtube.com/watch?v=jGwO_UgTS7I&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">the Machine Learning course from Stanford by Andrew Ng</a>, which is very good. 


      <!--h2>Github</h2>
      <p>All technical resources will be posted on the
        <a href="https://github.com/uob-COMS30035" target="_blank">COMS30035 Github organisation</a>. If you find any issues, please kindly raise an issue in the respective repository.
      </p-->

      <hr/>

    </div>
  </body>
</html>









